import sys, os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from api.semantic_search import search_faq
from models.mistral_model import model_response
from logs.logger import log_event

conversation_history = [] 


def chat_with_bot(prompt, faq_source):
    
   try:
      retrieved_answer, score = search_faq(query=prompt, faq_source=faq_source)
   except Exception as e:
      log_event("ERROR", f"an error happend while searching faqs: {e}")
   

   formatted_history = "\n".join([f"User: {msg['user']}\nBot: {msg['bot']}" for msg in conversation_history])

   if score < 1:
      relevance_score = "High Relevance"
   elif score < 1.5:
      relevance_score= "Medium Relevance"
   else:
      relevance_score = "Low Relevance"

   instructions = f"""
You are an AI assistant that helps users by engaging in conversation and answering support-related questions using a provided FAQ knowledge base.

üî∏ Your behavior is as follows:

1Ô∏è‚É£ **Small Talk & Casual Conversation**:
- If the user greets you or engages in casual conversation, respond naturally and warmly.
- You can chat like a friendly assistant ‚Äî jokes, comments, etc. ‚Äî as long as it's not giving factual answers.
- ‚úÖ Do **not** use the FAQ in these responses.

2Ô∏è‚É£ **FAQ-based Answering**:
- If the user asks a support, product, or policy-related question, use the **retrieved FAQ answer** only.
- Rephrase the FAQ answer in a natural way, but **do not add any new information**.
- ‚ùå Do **not** use external or general knowledge.
- If the FAQ answer doesn‚Äôt clearly answer the question, say:
  > "I‚Äôm sorry, I couldn‚Äôt find an answer to that in the FAQs."

3Ô∏è‚É£ **Conversation Context**:
- Use previous messages (below) to understand what the user is asking.
- Keep your tone friendly and concise.

---

### üí¨ Conversation History:
{formatted_history}

### ‚ùì User Question:
"{prompt}"

### üìÑ Retrieved FAQ Answer:
"{retrieved_answer}" (Relevance Score: {relevance_score})

---

### üó®Ô∏è Your Response:
"""
   
   try:
      response = model_response(instructions)
   except Exception as e:
      log_event("ERROR", f"an error occured while getting model response: {e}")
    
   conversation_history.append({"user": prompt, "bot": response})
   
   log_event("HISTORY", conversation_history)


   return response, score













